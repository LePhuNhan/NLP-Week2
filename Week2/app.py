from flask import Flask, render_template, request, jsonify
import spacy
import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
import json
import os
import re
import logging
from functools import lru_cache
from typing import Dict, List, Optional, Tuple, Union
from langdetect import detect, DetectorFactory
from pyvi import ViTokenizer, ViPosTagger
import underthesea

# Thi·∫øt l·∫≠p seed cho langdetect ƒë·ªÉ c√≥ k·∫øt qu·∫£ ·ªïn ƒë·ªãnh
DetectorFactory.seed = 0

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# T·∫£i d·ªØ li·ªáu NLTK c·∫ßn thi·∫øt
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('taggers/averaged_perceptron_tagger')
except LookupError:
    nltk.download('averaged_perceptron_tagger')

app = Flask(__name__)

# Cache cho k·∫øt qu·∫£ ph√¢n t√≠ch
analysis_cache = {}

# Kh·ªüi t·∫°o spaCy model
try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    print("SpaCy model 'en_core_web_sm' ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t.")
    print("Vui l√≤ng ch·∫°y: python -m spacy download en_core_web_sm")
    nlp = None

class TextAnalyzer:
    """L·ªõp ph√¢n t√≠ch vƒÉn b·∫£n s·ª≠ d·ª•ng NLTK, spaCy v√† th∆∞ vi·ªán ti·∫øng Vi·ªát"""
    
    def __init__(self):
        self.nlp = nlp
        self.cache = {}
        self.confidence_threshold = 0.7
        
    def validate_input(self, text: str) -> Tuple[bool, str]:
        """Validate input text"""
        if not text or not isinstance(text, str):
            return False, "VƒÉn b·∫£n kh√¥ng h·ª£p l·ªá"
        
        if len(text.strip()) < 2:
            return False, "VƒÉn b·∫£n qu√° ng·∫Øn"
        
        if len(text) > 10000:
            return False, "VƒÉn b·∫£n qu√° d√†i (t·ªëi ƒëa 10,000 k√Ω t·ª±)"
        
        return True, "OK"
    
    def calculate_confidence_score(self, entities: List[Dict], tokens: List[str]) -> float:
        """T√≠nh confidence score cho k·∫øt qu·∫£ ph√¢n t√≠ch"""
        if not entities or not tokens:
            return 0.0
        
        # T√≠nh score d·ª±a tr√™n s·ªë l∆∞·ª£ng entities v√† ƒë·ªô d√†i vƒÉn b·∫£n
        entity_ratio = len(entities) / len(tokens)
        base_score = min(entity_ratio * 10, 1.0)
        
        # Bonus cho entities c√≥ description chi ti·∫øt
        detailed_entities = sum(1 for e in entities if e.get('description', '') != e.get('label', ''))
        detail_bonus = detailed_entities / len(entities) * 0.2
        
        return min(base_score + detail_bonus, 1.0)
    
    @lru_cache(maxsize=128)
    def cached_detect_language(self, text: str) -> str:
        """Cached language detection"""
        return self.detect_language(text)
    
    def detect_language(self, text):
        """Ph√°t hi·ªán ng√¥n ng·ªØ c·ªßa vƒÉn b·∫£n"""
        try:
            # L·∫•y 100 k√Ω t·ª± ƒë·∫ßu ƒë·ªÉ ph√°t hi·ªán ng√¥n ng·ªØ nhanh h∆°n
            sample_text = text[:100] if len(text) > 100 else text
            detected_lang = detect(sample_text)
            return detected_lang
        except:
            return 'unknown'
    
    def tokenize_with_nltk(self, text):
        """Tokenization s·ª≠ d·ª•ng NLTK"""
        tokens = word_tokenize(text)
        return tokens
    
    def pos_tag_with_nltk(self, tokens):
        """POS tagging s·ª≠ d·ª•ng NLTK"""
        pos_tags = pos_tag(tokens)
        return pos_tags
    
    def tokenize_vietnamese(self, text):
        """Tokenization cho ti·∫øng Vi·ªát s·ª≠ d·ª•ng pyvi"""
        try:
            tokens = ViTokenizer.tokenize(text).split()
            return tokens
        except:
            # Fallback v·ªÅ word_tokenize n·∫øu pyvi l·ªói
            return word_tokenize(text)
    
    def pos_tag_vietnamese(self, text):
        """POS tagging cho ti·∫øng Vi·ªát s·ª≠ d·ª•ng pyvi"""
        try:
            pos_tags = ViPosTagger.postagging(ViTokenizer.tokenize(text))
            # K·∫øt h·ª£p tokens v√† pos tags
            tokens, tags = pos_tags
            pos_tags_list = list(zip(tokens, tags))
            
            # S·ª≠a c√°c POS tags sai
            corrected_tags = self.correct_vietnamese_pos_tags(pos_tags_list)
            return corrected_tags
        except:
            # Fallback v·ªÅ NLTK n·∫øu pyvi l·ªói
            tokens = word_tokenize(text)
            return pos_tag(tokens)
    
    def analyze_vietnamese_with_underthesea(self, text):
        """Ph√¢n t√≠ch ti·∫øng Vi·ªát s·ª≠ d·ª•ng underthesea"""
        try:
            # S·ª≠ d·ª•ng POS tags ƒë√£ ƒë∆∞·ª£c s·ª≠a t·ª´ pos_tag_vietnamese
            corrected_pos_tags = self.pos_tag_vietnamese(text)
            
            # Tokenization v√† POS tagging
            tokens_with_pos = []
            
            for token, pos in corrected_pos_tags:
                tokens_with_pos.append({
                    'token': token,
                    'pos': pos,
                    'tag': pos,  # underthesea ch·ªâ c√≥ POS, kh√¥ng c√≥ tag chi ti·∫øt
                    'lemma': token  # underthesea kh√¥ng c√≥ lemmatization
                })
            
            # Named Entity Recognition
            entities = []
            try:
                ner_results = underthesea.ner(text)
                
                # Underthesea tr·∫£ v·ªÅ (token, pos, chunk_tag, ner_tag)
                current_entity = ""
                current_label = ""
                
                for i, entity in enumerate(ner_results):
                    if len(entity) >= 4:  # (token, pos, chunk_tag, ner_tag)
                        token, pos, chunk_tag, ner_tag = entity[:4]
                        
                        if ner_tag.startswith('B-'):  # B·∫Øt ƒë·∫ßu entity m·ªõi
                            # L∆∞u entity tr∆∞·ªõc ƒë√≥ n·∫øu c√≥
                            if current_entity and current_label:
                                entities.append({
                                    'text': current_entity.strip(),
                                    'label': current_label,
                                    'start': 0,
                                    'end': 0,
                                    'description': self.get_vietnamese_ner_description(current_label)
                                })
                            
                            # B·∫Øt ƒë·∫ßu entity m·ªõi
                            current_entity = token
                            current_label = ner_tag[2:]  # B·ªè 'B-' prefix
                            
                        elif ner_tag.startswith('I-') and current_label == ner_tag[2:]:  # Ti·∫øp t·ª•c entity
                            # Ch·ªâ th√™m token n·∫øu kh√¥ng ph·∫£i d·∫•u c√¢u
                            if token not in [',', '.', '!', '?', ';', ':']:
                                current_entity += " " + token
                            
                        else:  # Kh√¥ng ph·∫£i entity ho·∫∑c k·∫øt th√∫c entity
                            # L∆∞u entity tr∆∞·ªõc ƒë√≥ n·∫øu c√≥
                            if current_entity and current_label:
                                entities.append({
                                    'text': current_entity.strip(),
                                    'label': current_label,
                                    'start': 0,
                                    'end': 0,
                                    'description': self.get_vietnamese_ner_description(current_label)
                                })
                                current_entity = ""
                                current_label = ""
                
                # L∆∞u entity cu·ªëi c√πng n·∫øu c√≥
                if current_entity and current_label:
                    entities.append({
                        'text': current_entity.strip(),
                        'label': current_label,
                        'start': 0,
                        'end': 0,
                        'description': self.get_vietnamese_ner_description(current_label)
                    })
                
                # L√†m s·∫°ch entities: lo·∫°i b·ªè entities qu√° ng·∫Øn ho·∫∑c ch·ªâ ch·ª©a d·∫•u c√¢u
                cleaned_entities = []
                for entity in entities:
                    # Lo·∫°i b·ªè d·∫•u c√¢u v√† kho·∫£ng tr·∫Øng ·ªü ƒë·∫ßu v√† cu·ªëi
                    clean_text = entity['text'].strip().strip(',.!?;:').strip()
                    if len(clean_text) > 1:
                        entity['text'] = clean_text
                        
                        # S·ª≠a nh√£n sai d·ª±a tr√™n context v√† t·ª´ kh√≥a
                        entity = self.correct_vietnamese_ner_labels(entity)
                        
                        cleaned_entities.append(entity)
                
                # Th√™m c√°c entities b·ªã thi·∫øu
                additional_entities = self.add_missing_vietnamese_entities(text, cleaned_entities)
                entities = cleaned_entities + additional_entities
                    
            except Exception as e:
                pass  # NER c√≥ th·ªÉ kh√¥ng ho·∫°t ƒë·ªông v·ªõi m·ªôt s·ªë phi√™n b·∫£n
            
            return {
                'tokens_with_pos': tokens_with_pos,
                'entities': entities
            }
        except Exception as e:
            print(f"L·ªói khi ph√¢n t√≠ch ti·∫øng Vi·ªát v·ªõi underthesea: {e}")
            return None
    
    def get_vietnamese_ner_description(self, ner_tag):
        """L·∫•y m√¥ t·∫£ cho NER tag ti·∫øng Vi·ªát"""
        descriptions = {
            'PER': 'T√™n ng∆∞·ªùi',
            'LOC': 'ƒê·ªãa ƒëi·ªÉm',
            'ORG': 'T·ªï ch·ª©c',
            'MISC': 'Kh√°c',
            'NP': 'C·ª•m danh t·ª´',
            'DATE': 'Ng√†y th√°ng',
            'O': 'Kh√¥ng ph·∫£i entity'
        }
        return descriptions.get(ner_tag, ner_tag)
    
    def get_vietnamese_pos_description(self, pos_tag):
        """L·∫•y m√¥ t·∫£ chi ti·∫øt cho POS tag ti·∫øng Vi·ªát"""
        descriptions = {
            # Danh t·ª´
            'N': 'Danh t·ª´ chung (Noun)',
            'Np': 'Danh t·ª´ ri√™ng (Proper Noun)',
            'Nu': 'Danh t·ª´ ƒë∆°n v·ªã (Unit Noun)',
            'Nc': 'Danh t·ª´ ch·ªâ lo·∫°i (Classifier Noun)',
            
            # ƒê·ªông t·ª´
            'V': 'ƒê·ªông t·ª´ (Verb)',
            'Vb': 'ƒê·ªông t·ª´ b·ªï tr·ª£ (Auxiliary Verb)',
            'Vv': 'ƒê·ªông t·ª´ v·ªã ng·ªØ (Predicative Verb)',
            
            # T√≠nh t·ª´
            'A': 'T√≠nh t·ª´ (Adjective)',
            'Ab': 'T√≠nh t·ª´ b·ªï tr·ª£ (Auxiliary Adjective)',
            
            # ƒê·∫°i t·ª´
            'P': 'ƒê·∫°i t·ª´ (Pronoun)',
            'Pp': 'ƒê·∫°i t·ª´ nh√¢n x∆∞ng (Personal Pronoun)',
            'Pd': 'ƒê·∫°i t·ª´ ch·ªâ ƒë·ªãnh (Demonstrative Pronoun)',
            'Pq': 'ƒê·∫°i t·ª´ nghi v·∫•n (Interrogative Pronoun)',
            
            # S·ªë t·ª´
            'M': 'S·ªë t·ª´ (Numeral)',
            'Mc': 'S·ªë t·ª´ ch·ªâ s·ªë l∆∞·ª£ng (Cardinal Numeral)',
            'Mo': 'S·ªë t·ª´ th·ª© t·ª± (Ordinal Numeral)',
            
            # Ph√≥ t·ª´
            'R': 'Ph√≥ t·ª´ (Adverb)',
            'Rg': 'Ph√≥ t·ª´ ch·ªâ m·ª©c ƒë·ªô (Degree Adverb)',
            'Rr': 'Ph√≥ t·ª´ ch·ªâ th·ªùi gian (Time Adverb)',
            'Rs': 'Ph√≥ t·ª´ ch·ªâ n∆°i ch·ªën (Place Adverb)',
            
            # Gi·ªõi t·ª´
            'E': 'Gi·ªõi t·ª´ (Preposition)',
            'Ec': 'Gi·ªõi t·ª´ ch·ªâ n∆°i ch·ªën (Place Preposition)',
            'Et': 'Gi·ªõi t·ª´ ch·ªâ th·ªùi gian (Time Preposition)',
            
            # Li√™n t·ª´
            'C': 'Li√™n t·ª´ (Conjunction)',
            'Cc': 'Li√™n t·ª´ k·∫øt h·ª£p (Coordinating Conjunction)',
            'Cs': 'Li√™n t·ª´ ph·ª• thu·ªôc (Subordinating Conjunction)',
            
            # Th√°n t·ª´
            'I': 'Th√°n t·ª´ (Interjection)',
            
            # Tr·ª£ t·ª´
            'T': 'Tr·ª£ t·ª´ (Particle)',
            'Td': 'Tr·ª£ t·ª´ ƒë·ªãnh ng·ªØ (Determiner Particle)',
            'Tg': 'Tr·ª£ t·ª´ ng·ªØ kh√≠ (Modal Particle)',
            
            # D·∫•u c√¢u
            'CH': 'D·∫•u c√¢u (Punctuation)',
            'CHp': 'D·∫•u ch·∫•m (Period)',
            'CHc': 'D·∫•u ph·∫©y (Comma)',
            'CHh': 'D·∫•u h·ªèi (Question Mark)',
            'CHk': 'D·∫•u ch·∫•m than (Exclamation Mark)',
            
            # T·ª´ ngo·∫°i lai
            'FW': 'T·ª´ ngo·∫°i lai (Foreign Word)',
            
            # Kh√°c
            'X': 'T·ª´ kh√°c (Other)',
            'Y': 'T·ª´ vi·∫øt t·∫Øt (Abbreviation)',
            'Z': 'T·ª´ kh√¥ng x√°c ƒë·ªãnh (Unknown)'
        }
        return descriptions.get(pos_tag, f'{pos_tag} (Kh√¥ng x√°c ƒë·ªãnh)')
    
    def correct_vietnamese_ner_labels(self, entity):
        """S·ª≠a c√°c nh√£n NER sai d·ª±a tr√™n context v√† t·ª´ kh√≥a"""
        text = entity['text'].lower()
        current_label = entity['label']
        
        # Danh s√°ch c√°c c√¥ng ty n·ªïi ti·∫øng
        company_names = ['apple', 'microsoft', 'google', 'amazon', 'facebook', 'tesla', 'samsung', 'sony', 'nike', 'adidas']
        
        # Danh s√°ch c√°c t√™n ng∆∞·ªùi n·ªïi ti·∫øng
        famous_people = ['steve jobs', 'steve wozniak', 'ronald wayne', 'tim cook', 'bill gates', 'paul allen', 
                        'mark zuckerberg', 'jeff bezos', 'elon musk', 'larry page', 'sergey brin']
        
        # S·ª≠a Apple t·ª´ PER th√†nh ORG
        if text == 'apple' and current_label == 'PER':
            entity['label'] = 'ORG'
            entity['description'] = self.get_vietnamese_ner_description('ORG')
        
        # S·ª≠a c√°c t√™n ng∆∞·ªùi n·ªïi ti·∫øng t·ª´ LOC th√†nh PER
        elif text in famous_people and current_label == 'LOC':
            entity['label'] = 'PER'
            entity['description'] = self.get_vietnamese_ner_description('PER')
        
        # S·ª≠a nƒÉm t·ª´ LOC th√†nh DATE
        elif ('nƒÉm' in text or text.isdigit()) and current_label == 'LOC':
            # Ki·ªÉm tra n·∫øu l√† nƒÉm (4 ch·ªØ s·ªë)
            if text.replace('nƒÉm ', '').isdigit() and len(text.replace('nƒÉm ', '')) == 4:
                entity['label'] = 'DATE'
                entity['description'] = self.get_vietnamese_ner_description('DATE')
        
        # S·ª≠a c√°c c√¥ng ty kh√°c t·ª´ PER th√†nh ORG
        elif text in company_names and current_label == 'PER':
            entity['label'] = 'ORG'
            entity['description'] = self.get_vietnamese_ner_description('ORG')
        
        # S·ª≠a b·ªánh vi·ªán t·ª´ PER th√†nh ORG
        elif 'ch·ª£ r·∫´y' in text and current_label == 'PER':
            entity['label'] = 'ORG'
            entity['description'] = self.get_vietnamese_ner_description('ORG')
        
        # S·ª≠a TP.HCM t·ª´ PER th√†nh LOC
        elif ('tp.hcm' in text or 'h·ªì ch√≠ minh' in text) and current_label == 'PER':
            entity['label'] = 'LOC'
            entity['description'] = self.get_vietnamese_ner_description('LOC')
        
        # S·ª≠a b·ªánh vi·ªán t·ª´ PER th√†nh ORG
        elif 'b·ªánh vi·ªán' in text and current_label == 'PER':
            entity['label'] = 'ORG'
            entity['description'] = self.get_vietnamese_ner_description('ORG')
        
        # S·ª≠a tr∆∞·ªùng ƒë·∫°i h·ªçc t·ª´ LOC th√†nh ORG
        elif ('ƒë·∫°i h·ªçc' in text or 'b√°ch khoa' in text or 'h·ªçc vi·ªán' in text) and current_label == 'LOC':
            entity['label'] = 'ORG'
            entity['description'] = self.get_vietnamese_ner_description('ORG')
        
        # S·ª≠a "Anh" t·ª´ PER th√†nh MISC (ƒë·∫°i t·ª´)
        elif text == 'anh' and current_label == 'PER':
            entity['label'] = 'MISC'
            entity['description'] = self.get_vietnamese_ner_description('MISC')
        
        # S·ª≠a t√™n hu·∫•n luy·ªán vi√™n t·ª´ LOC th√†nh PER
        elif 'park hang-seo' in text and current_label == 'LOC':
            entity['label'] = 'PER'
            entity['description'] = self.get_vietnamese_ner_description('PER')
        
        # S·ª≠a hu·∫•n luy·ªán vi√™n t·ª´ LOC th√†nh MISC
        elif 'hu·∫•n luy·ªán vi√™n' in text and current_label == 'LOC':
            entity['label'] = 'MISC'
            entity['description'] = self.get_vietnamese_ner_description('MISC')
        
        # S·ª≠a FPT Software t·ª´ PER th√†nh ORG
        elif 'fpt software' in text and current_label == 'PER':
            entity['label'] = 'ORG'
            entity['description'] = self.get_vietnamese_ner_description('ORG')
        
        # S·ª≠a CEO t·ª´ LOC th√†nh MISC
        elif text == 'ceo' and current_label == 'LOC':
            entity['label'] = 'MISC'
            entity['description'] = self.get_vietnamese_ner_description('MISC')
        
        # S·ª≠a c√°c c√¥ng ty kh√°c t·ª´ PER th√†nh ORG
        elif any(company in text for company in ['vng', 'vietcombank', 'fpt', 'vinfast', 'vingroup']) and current_label == 'PER':
            entity['label'] = 'ORG'
            entity['description'] = self.get_vietnamese_ner_description('ORG')
        
        # S·ª≠a c√°c tr∆∞·ªùng ƒë·∫°i h·ªçc t·ª´ LOC th√†nh ORG
        elif any(uni in text for uni in ['khoa h·ªçc t·ª± nhi√™n', 'b√°ch khoa', 'qu·ªëc gia']) and current_label == 'LOC':
            entity['label'] = 'ORG'
            entity['description'] = self.get_vietnamese_ner_description('ORG')
        
        # S·ª≠a ng√¢n h√†ng t·ª´ PER th√†nh ORG
        elif 'ng√¢n h√†ng' in text and current_label == 'PER':
            entity['label'] = 'ORG'
            entity['description'] = self.get_vietnamese_ner_description('ORG')
        
        # S·ª≠a c√°c ch·ª©c v·ª• t·ª´ LOC th√†nh MISC
        elif any(title in text for title in ['hi·ªáu tr∆∞·ªüng', 'ch·ªß t·ªãch', 'gi√°m ƒë·ªëc', 'th·ªß t∆∞·ªõng', 't·ªïng th·ªëng']) and current_label == 'LOC':
            entity['label'] = 'MISC'
            entity['description'] = self.get_vietnamese_ner_description('MISC')
        
        # S·ª≠a c√°c ƒë·ªãa ƒëi·ªÉm t·ª´ PER th√†nh LOC
        elif any(location in text for location in ['ƒë√¥ng nam √°', 'th√†nh ph·ªë h·ªì ch√≠ minh', 'hoa k·ª≥']) and current_label == 'PER':
            entity['label'] = 'LOC'
            entity['description'] = self.get_vietnamese_ner_description('LOC')
        
        return entity
    
    def add_missing_vietnamese_entities(self, text, existing_entities):
        """Th√™m c√°c entities b·ªã thi·∫øu d·ª±a tr√™n t·ª´ kh√≥a v√† pattern"""
        additional_entities = []
        text_lower = text.lower()
        
        # Danh s√°ch c√°c t√™n ng∆∞·ªùi ph·ªï bi·∫øn
        common_names = ['ki√™n', 'minh', 'h√πng', 'd≈©ng', 'tu·∫•n', 'nam', 'linh', 'hoa', 'mai', 'lan', 
                       'th·∫£o', 'ng·ªçc', 'vy', 'anh', 'huy', 'ƒë·ª©c', 'quang', 'phong', 'long', 'kh√°nh']
        
        # Danh s√°ch c√°c tr∆∞·ªùng ƒë·∫°i h·ªçc
        universities = ['hcmute', 'hcmus', 'hcmut', 'hust', 'uet', 'neu', 'ftu', 'hue', 'dut', 'ctu']
        
        # Danh s√°ch c√°c qu·∫≠n/huy·ªán
        districts = ['qu·∫≠n 1', 'qu·∫≠n 2', 'qu·∫≠n 3', 'qu·∫≠n 4', 'qu·∫≠n 5', 'qu·∫≠n 6', 'qu·∫≠n 7', 'qu·∫≠n 8', 
                    'qu·∫≠n 9', 'qu·∫≠n 10', 'qu·∫≠n 11', 'qu·∫≠n 12', 'qu·∫≠n b√¨nh th·∫°nh', 'qu·∫≠n g√≤ v·∫•p', 
                    'qu·∫≠n ph√∫ nhu·∫≠n', 'qu·∫≠n t√¢n b√¨nh', 'qu·∫≠n t√¢n ph√∫', 'qu·∫≠n th·ªß ƒë·ª©c']
        
        # Danh s√°ch c√°c b·∫±ng c·∫•p
        degrees = ['th·∫°c sƒ©', 'ti·∫øn sƒ©', 'c·ª≠ nh√¢n', 'k·ªπ s∆∞', 'b√°c sƒ©', 'th·∫°c s·ªπ', 'ti·∫øn s·ªπ']
        
        # Danh s√°ch c√°c ƒë∆°n v·ªã ƒëo l∆∞·ªùng
        units = ['tu·ªïi', 'nƒÉm', 'th√°ng', 'ng√†y', 'gi·ªù', 'ph√∫t', 'gi√¢y', 'kg', 'g', 'm', 'cm', 'km', 'l√≠t', 'ml']
        
        # Ki·ªÉm tra t√™n ng∆∞·ªùi
        for name in common_names:
            if name in text_lower and not any(name in entity['text'].lower() for entity in existing_entities):
                additional_entities.append({
                    'text': name.title(),
                    'label': 'PER',
                    'start': 0,
                    'end': 0,
                    'description': 'T√™n ng∆∞·ªùi'
                })
        
        # Ki·ªÉm tra s·ªë tu·ªïi (s·ªë + tu·ªïi)
        import re
        age_pattern = r'(\d+)\s*tu·ªïi'
        age_matches = re.findall(age_pattern, text_lower)
        for age in age_matches:
            if not any(age in entity['text'] for entity in existing_entities):
                additional_entities.append({
                    'text': age,
                    'label': 'NUM',
                    'start': 0,
                    'end': 0,
                    'description': 'S·ªë tu·ªïi'
                })
        
        # Ki·ªÉm tra ƒë∆°n v·ªã ƒëo l∆∞·ªùng (ch·ªâ nh·ªØng t·ª´ c√≥ √Ω nghƒ©a trong ng·ªØ c·∫£nh)
        for unit in units:
            if unit in text_lower and not any(unit in entity['text'].lower() for entity in existing_entities):
                # Ch·ªâ th√™m n·∫øu l√† t·ª´ c√≥ ƒë·ªô d√†i > 1 ho·∫∑c l√† ƒë∆°n v·ªã ph·ªï bi·∫øn
                if len(unit) > 1:
                    additional_entities.append({
                        'text': unit.title(),
                        'label': 'MISC',
                        'start': 0,
                        'end': 0,
                        'description': 'ƒê∆°n v·ªã ƒëo l∆∞·ªùng'
                    })
                # Ch·ªâ th√™m ƒë∆°n v·ªã 1 k√Ω t·ª± n·∫øu c√≥ s·ªë ƒë·ª©ng tr∆∞·ªõc (v√≠ d·ª•: "5g", "10m")
                elif len(unit) == 1:
                    import re
                    # Ki·ªÉm tra xem c√≥ s·ªë ƒë·ª©ng tr∆∞·ªõc kh√¥ng
                    pattern = r'\d+\s*' + unit
                    if re.search(pattern, text_lower):
                        additional_entities.append({
                            'text': unit.upper(),
                            'label': 'MISC',
                            'start': 0,
                            'end': 0,
                            'description': 'ƒê∆°n v·ªã ƒëo l∆∞·ªùng'
                        })
        
        # Ki·ªÉm tra tr∆∞·ªùng ƒë·∫°i h·ªçc
        for uni in universities:
            if uni in text_lower and not any(uni in entity['text'].lower() for entity in existing_entities):
                additional_entities.append({
                    'text': uni.upper(),
                    'label': 'ORG',
                    'start': 0,
                    'end': 0,
                    'description': 'T·ªï ch·ª©c'
                })
        
        # Ki·ªÉm tra qu·∫≠n/huy·ªán
        for district in districts:
            if district in text_lower and not any(district in entity['text'].lower() for entity in existing_entities):
                additional_entities.append({
                    'text': district.title(),
                    'label': 'LOC',
                    'start': 0,
                    'end': 0,
                    'description': 'ƒê·ªãa ƒëi·ªÉm'
                })
        
        # Ki·ªÉm tra b·∫±ng c·∫•p
        for degree in degrees:
            if degree in text_lower and not any(degree in entity['text'].lower() for entity in existing_entities):
                additional_entities.append({
                    'text': degree.title(),
                    'label': 'MISC',
                    'start': 0,
                    'end': 0,
                    'description': 'Kh√°c'
                })
        
        # Ki·ªÉm tra b·ªánh vi·ªán
        if 'b·ªánh vi·ªán' in text_lower and not any('b·ªánh vi·ªán' in entity['text'].lower() for entity in existing_entities):
            additional_entities.append({
                'text': 'B·ªánh vi·ªán',
                'label': 'ORG',
                'start': 0,
                'end': 0,
                'description': 'T·ªï ch·ª©c'
            })
        
        # Ki·ªÉm tra s·ªë gi∆∞·ªùng b·ªánh (s·ªë + gi∆∞·ªùng b·ªánh)
        bed_pattern = r'(\d+[.,]?\d*)\s*gi∆∞·ªùng\s*b·ªánh'
        bed_matches = re.findall(bed_pattern, text_lower)
        for bed in bed_matches:
            if not any(bed in entity['text'] for entity in existing_entities):
                additional_entities.append({
                    'text': bed,
                    'label': 'NUM',
                    'start': 0,
                    'end': 0,
                    'description': 'S·ªë l∆∞·ª£ng'
                })
        
        # Ki·ªÉm tra gi∆∞·ªùng b·ªánh
        if 'gi∆∞·ªùng b·ªánh' in text_lower and not any('gi∆∞·ªùng b·ªánh' in entity['text'].lower() for entity in existing_entities):
            additional_entities.append({
                'text': 'gi∆∞·ªùng b·ªánh',
                'label': 'MISC',
                'start': 0,
                'end': 0,
                'description': 'ƒê∆°n v·ªã ƒëo l∆∞·ªùng'
            })
        
        # Ki·ªÉm tra tr∆∞·ªùng ƒë·∫°i h·ªçc
        if 'ƒë·∫°i h·ªçc' in text_lower and not any('ƒë·∫°i h·ªçc' in entity['text'].lower() for entity in existing_entities):
            additional_entities.append({
                'text': 'ƒê·∫°i h·ªçc',
                'label': 'ORG',
                'start': 0,
                'end': 0,
                'description': 'T·ªï ch·ª©c'
            })
        
        # Ki·ªÉm tra t√™n ng∆∞·ªùi ƒë·∫ßy ƒë·ªß (Nguy·ªÖn VƒÉn Minh)
        import re
        full_name_pattern = r'(nguy·ªÖn|tr·∫ßn|l√™|ph·∫°m|ho√†ng|phan|v≈©|v√µ|ƒë·∫∑ng|b√πi|ƒë·ªó|h·ªì|ng√¥|d∆∞∆°ng|l√Ω)\s+(vƒÉn|th·ªã|ƒë·ª©c|minh|h√πng|d≈©ng|tu·∫•n|nam|linh|hoa|mai|lan|th·∫£o|ng·ªçc|vy|anh|huy|ƒë·ª©c|quang|phong|long|kh√°nh)'
        full_name_matches = re.findall(full_name_pattern, text_lower)
        for first_name, middle_name in full_name_matches:
            full_name = f"{first_name.title()} {middle_name.title()}"
            if not any(full_name.lower() in entity['text'].lower() for entity in existing_entities):
                additional_entities.append({
                    'text': full_name,
                    'label': 'PER',
                    'start': 0,
                    'end': 0,
                    'description': 'T√™n ng∆∞·ªùi'
                })
        
        # Ki·ªÉm tra t√™n ng∆∞·ªùi n∆∞·ªõc ngo√†i (Park Hang-seo)
        foreign_name_pattern = r'(park|kim|lee|choi|jung|yoon|kang|lim|oh|seo)\s+(hang-seo|min-jae|son|heung-min|jae-sung|woo-young|hyun-jin|dong-gook|bo-kyung|young-pyo)'
        foreign_name_matches = re.findall(foreign_name_pattern, text_lower)
        for first_name, last_name in foreign_name_matches:
            full_name = f"{first_name.title()} {last_name.title()}"
            if not any(full_name.lower() in entity['text'].lower() for entity in existing_entities):
                additional_entities.append({
                    'text': full_name,
                    'label': 'PER',
                    'start': 0,
                    'end': 0,
                    'description': 'T√™n ng∆∞·ªùi'
                })
        
        # Ki·ªÉm tra t·ª∑ s·ªë (2-1, 3-0, 1-1)
        score_pattern = r'(\d+)\s*-\s*(\d+)'
        score_matches = re.findall(score_pattern, text_lower)
        for score1, score2 in score_matches:
            score = f"{score1}-{score2}"
            if not any(score in entity['text'] for entity in existing_entities):
                additional_entities.append({
                    'text': score,
                    'label': 'NUM',
                    'start': 0,
                    'end': 0,
                    'description': 'T·ª∑ s·ªë'
                })
        
        # Ki·ªÉm tra hu·∫•n luy·ªán vi√™n
        if 'hu·∫•n luy·ªán vi√™n' in text_lower and not any('hu·∫•n luy·ªán vi√™n' in entity['text'].lower() for entity in existing_entities):
            additional_entities.append({
                'text': 'hu·∫•n luy·ªán vi√™n',
                'label': 'MISC',
                'start': 0,
                'end': 0,
                'description': 'Ch·ª©c v·ª•'
            })
        
        # Ki·ªÉm tra c√¥ng ty
        if 'c√¥ng ty' in text_lower and not any('c√¥ng ty' in entity['text'].lower() for entity in existing_entities):
            additional_entities.append({
                'text': 'C√¥ng ty',
                'label': 'ORG',
                'start': 0,
                'end': 0,
                'description': 'T·ªï ch·ª©c'
            })
        
        # Ki·ªÉm tra CEO
        if 'ceo' in text_lower and not any('ceo' in entity['text'].lower() for entity in existing_entities):
            additional_entities.append({
                'text': 'CEO',
                'label': 'MISC',
                'start': 0,
                'end': 0,
                'description': 'Ch·ª©c v·ª•'
            })
        
        # Ki·ªÉm tra s·ªë nƒÉm (1999, 2000, 2023...)
        year_pattern = r'\b(19|20)\d{2}\b'
        year_matches = re.findall(year_pattern, text_lower)
        for year in year_matches:
            if not any(year in entity['text'] for entity in existing_entities):
                additional_entities.append({
                    'text': year,
                    'label': 'NUM',
                    'start': 0,
                    'end': 0,
                    'description': 'NƒÉm'
                })
        
        # Ki·ªÉm tra c√°c ch·ª©c v·ª•
        titles = ['hi·ªáu tr∆∞·ªüng', 'ch·ªß t·ªãch', 'gi√°m ƒë·ªëc', 'th·ªß t∆∞·ªõng', 't·ªïng th·ªëng', 'pgs.ts', 'bs.']
        for title in titles:
            if title in text_lower and not any(title in entity['text'].lower() for entity in existing_entities):
                additional_entities.append({
                    'text': title.title(),
                    'label': 'MISC',
                    'start': 0,
                    'end': 0,
                    'description': 'Ch·ª©c v·ª•'
                })
        
        # Ki·ªÉm tra ng√¢n h√†ng
        if 'ng√¢n h√†ng' in text_lower and not any('ng√¢n h√†ng' in entity['text'].lower() for entity in existing_entities):
            additional_entities.append({
                'text': 'Ng√¢n h√†ng',
                'label': 'ORG',
                'start': 0,
                'end': 0,
                'description': 'T·ªï ch·ª©c'
            })
        
        # Ki·ªÉm tra tr∆∞·ªùng ƒë·∫°i h·ªçc
        universities = ['khoa h·ªçc t·ª± nhi√™n', 'b√°ch khoa', 'qu·ªëc gia']
        for uni in universities:
            if uni in text_lower and not any(uni in entity['text'].lower() for entity in existing_entities):
                additional_entities.append({
                    'text': uni.title(),
                    'label': 'ORG',
                    'start': 0,
                    'end': 0,
                    'description': 'T·ªï ch·ª©c'
                })
        
        # Ki·ªÉm tra d√¢n s·ªë (97 tri·ªáu, 8 tri·ªáu...)
        population_pattern = r'(\d+)\s*(tri·ªáu|ngh√¨n|t·ª∑)'
        population_matches = re.findall(population_pattern, text_lower)
        for number, unit in population_matches:
            population = f"{number} {unit}"
            if not any(population in entity['text'] for entity in existing_entities):
                additional_entities.append({
                    'text': population,
                    'label': 'NUM',
                    'start': 0,
                    'end': 0,
                    'description': 'D√¢n s·ªë'
                })
        
        # Ki·ªÉm tra c√°c ƒë·ªãa ƒëi·ªÉm ƒë·∫∑c bi·ªát
        special_locations = ['ƒë√¥ng nam √°', 'th√†nh ph·ªë h·ªì ch√≠ minh', 'hoa k·ª≥', 'washington d.c.', 'boston']
        for location in special_locations:
            if location in text_lower and not any(location in entity['text'].lower() for entity in existing_entities):
                additional_entities.append({
                    'text': location.title(),
                    'label': 'LOC',
                    'start': 0,
                    'end': 0,
                    'description': 'ƒê·ªãa ƒëi·ªÉm'
                })
        
        return additional_entities
    
    def correct_vietnamese_pos_tags(self, pos_tags):
        """S·ª≠a c√°c POS tags sai d·ª±a tr√™n context v√† t·ª´ kh√≥a"""
        corrected_tags = []
        
        for i, (token, pos) in enumerate(pos_tags):
            # Danh s√°ch c√°c t√™n ng∆∞·ªùi ph·ªï bi·∫øn
            common_names = ['ki√™n', 'minh', 'h√πng', 'd≈©ng', 'tu·∫•n', 'nam', 'linh', 'hoa', 'mai', 'lan', 
                          'th·∫£o', 'ng·ªçc', 'vy', 'anh', 'huy', 'ƒë·ª©c', 'quang', 'phong', 'long', 'kh√°nh']
            
            # Danh s√°ch c√°c t·ª´ c√≥ th·ªÉ l√† ph√≥ t·ª´ ch·ªâ th·ªùi gian
            time_adverbs = ['hi·ªán_t·∫°i', 'hi·ªán_nay', 'b√¢y_gi·ªù', 'l√∫c_n√†y', 'ngay_b√¢y_gi·ªù', 'hi·ªán_gi·ªù']
            
            # S·ª≠a t√™n ng∆∞·ªùi t·ª´ N th√†nh Np
            if token.lower() in common_names and pos == 'N':
                corrected_tags.append((token, 'Np'))
            
            # S·ª≠a hi·ªán_t·∫°i t·ª´ N th√†nh R trong ng·ªØ c·∫£nh ph√π h·ª£p
            elif token.lower() == 'hi·ªán_t·∫°i' and pos == 'N':
                # Ki·ªÉm tra ng·ªØ c·∫£nh xung quanh
                context_around = []
                for j in range(max(0, i-2), min(len(pos_tags), i+3)):
                    if j != i:
                        context_around.append(pos_tags[j][0].lower())
                
                context_text = ' '.join(context_around)
                
                # N·∫øu c√≥ t·ª´ "tu·ªïi" ho·∫∑c "nƒÉm" g·∫ßn ƒë√≥, c√≥ th·ªÉ l√† ph√≥ t·ª´
                if 'tu·ªïi' in context_text or 'nƒÉm' in context_text:
                    corrected_tags.append((token, 'R'))
                else:
                    corrected_tags.append((token, pos))
            
            # Gi·ªØ nguy√™n c√°c tags kh√°c
            else:
                corrected_tags.append((token, pos))
        
        return corrected_tags
    
    def correct_english_ner_labels(self, entity):
        """S·ª≠a c√°c nh√£n NER ti·∫øng Anh sai d·ª±a tr√™n context v√† t·ª´ kh√≥a"""
        text = entity['text'].lower()
        current_label = entity['label']
        
        # S·ª≠a c√°c l·ªói ph·ªï bi·∫øn cho ti·∫øng Anh
        if 'mvp' in text and current_label == 'ORG':
            entity['label'] = 'MISC'
            entity['description'] = 'Award/Title'
        elif 'championship' in text and current_label == 'ORG':
            entity['label'] = 'EVENT'
            entity['description'] = 'Sports event'
        elif 'finals' in text and current_label == 'ORG':
            entity['label'] = 'EVENT'
            entity['description'] = 'Sports event'
        elif 'nba' in text and current_label == 'PERSON':
            entity['label'] = 'ORG'
            entity['description'] = 'Sports organization'
        elif 'ai' in text and current_label == 'PERSON':
            entity['label'] = 'MISC'
            entity['description'] = 'Technology'
        elif 'software engineer' in text and current_label == 'PERSON':
            entity['label'] = 'MISC'
            entity['description'] = 'Job title'
        
        return entity
    
    def add_missing_english_entities(self, text, existing_entities):
        """Th√™m c√°c entities b·ªã thi·∫øu cho ti·∫øng Anh"""
        additional_entities = []
        text_lower = text.lower()
        
        import re
        
        # Ki·ªÉm tra c√°c ch·ª©c v·ª•
        job_titles = ['ceo', 'president', 'coach', 'mvp', 'software engineer', 'champion']
        for title in job_titles:
            if title in text_lower and not any(title in entity['text'].lower() for entity in existing_entities):
                additional_entities.append({
                    'text': title.title(),
                    'label': 'MISC',
                    'start': 0,
                    'end': 0,
                    'description': 'Job title'
                })
        
        # Ki·ªÉm tra c√°c s·ª± ki·ªán th·ªÉ thao
        sports_events = ['championship', 'finals', 'nba']
        for event in sports_events:
            if event in text_lower and not any(event in entity['text'].lower() for entity in existing_entities):
                if event == 'nba':
                    additional_entities.append({
                        'text': 'NBA',
                        'label': 'ORG',
                        'start': 0,
                        'end': 0,
                        'description': 'Sports organization'
                    })
                else:
                    additional_entities.append({
                        'text': event.title(),
                        'label': 'EVENT',
                        'start': 0,
                        'end': 0,
                        'description': 'Sports event'
                    })
        
        # Ki·ªÉm tra c√¥ng ngh·ªá
        if 'ai' in text_lower and not any('ai' in entity['text'].lower() for entity in existing_entities):
            additional_entities.append({
                'text': 'AI',
                'label': 'MISC',
                'start': 0,
                'end': 0,
                'description': 'Technology'
            })
        
        return additional_entities
    
    def analyze_with_spacy(self, text):
        """Ph√¢n t√≠ch vƒÉn b·∫£n s·ª≠ d·ª•ng spaCy"""
        if not self.nlp:
            return None
        
        doc = self.nlp(text)
        
        # Tokenization v√† POS tagging
        tokens_with_pos = []
        for token in doc:
            tokens_with_pos.append({
                'token': token.text,
                'pos': token.pos_,
                'tag': token.tag_,
                'lemma': token.lemma_
            })
        
        # Named Entity Recognition
        entities = []
        for ent in doc.ents:
            entity = {
                'text': ent.text,
                'label': ent.label_,
                'start': ent.start_char,
                'end': ent.end_char,
                'description': spacy.explain(ent.label_)
            }
            
            # S·ª≠a c√°c nh√£n NER sai
            entity = self.correct_english_ner_labels(entity)
            entities.append(entity)
        
        # Th√™m c√°c entities b·ªã thi·∫øu
        additional_entities = self.add_missing_english_entities(text, entities)
        entities.extend(additional_entities)
        
        return {
            'tokens_with_pos': tokens_with_pos,
            'entities': entities
        }
    
    def analyze_mixed_language_text(self, text):
        """Ph√¢n t√≠ch vƒÉn b·∫£n h·ªón h·ª£p (ti·∫øng Vi·ªát + ti·∫øng Anh)"""
        import re
        
        # T√¨m c√°c t·ª´ ti·∫øng Anh trong vƒÉn b·∫£n
        english_words = re.findall(r'\b[A-Za-z]+\b', text)
        vietnamese_words = re.findall(r'[√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒëƒê]+\w*', text)
        
        # T√≠nh t·ªïng s·ªë t·ª´
        total_words = len(english_words) + len(vietnamese_words)
        if total_words == 0:
            return None
        
        # T√≠nh t·ª∑ l·ªá
        english_ratio = len(english_words) / total_words
        vietnamese_ratio = len(vietnamese_words) / total_words
        
        # Ch·ªâ coi l√† mixed khi:
        # 1. C√≥ √≠t nh·∫•t 5 t·ª´ ti·∫øng Anh
        # 2. C√≥ √≠t nh·∫•t 5 t·ª´ ti·∫øng Vi·ªát  
        # 3. Ti·∫øng Anh chi·∫øm 30-70% vƒÉn b·∫£n
        # 4. Kh√¥ng ph·∫£i ch·ªâ l√† t√™n ri√™ng ho·∫∑c t·ª´ vi·∫øt t·∫Øt
        if (len(english_words) >= 5 and len(vietnamese_words) >= 5 and 
            0.3 <= english_ratio <= 0.7):
            
            # Ki·ªÉm tra xem c√≥ ph·∫£i ch·ªâ l√† t√™n ri√™ng/t·ª´ vi·∫øt t·∫Øt kh√¥ng
            common_vietnamese_words = ['c√¥ng', 'ty', 'c√≥', 'tr·ª•', 's·ªü', 't·∫°i', 'hi·ªán', 't·∫°i', 'l√†', 'ƒë∆∞·ª£c', 'th√†nh', 'l·∫≠p', 'v√†o', 'nƒÉm']
            common_english_words = ['the', 'is', 'are', 'was', 'were', 'have', 'has', 'had', 'will', 'would', 'can', 'could', 'should', 'may', 'might']
            
            # ƒê·∫øm t·ª´ ti·∫øng Anh th√¥ng th∆∞·ªùng
            meaningful_english = sum(1 for word in english_words if word.lower() in common_english_words)
            
            # N·∫øu √≠t h∆°n 2 t·ª´ ti·∫øng Anh c√≥ nghƒ©a, coi nh∆∞ ti·∫øng Vi·ªát
            if meaningful_english < 2:
                return None
            
            # S·ª≠ d·ª•ng NLTK cho tokenization
            nltk_tokens = self.tokenize_with_nltk(text)
            nltk_pos_tags = self.pos_tag_with_nltk(nltk_tokens)
            
            # S·ª≠ d·ª•ng spaCy cho NER (t·ªët h∆°n cho ti·∫øng Anh)
            spacy_analysis = self.analyze_with_spacy(text)
            
            # Th√™m logic correction cho vƒÉn b·∫£n h·ªón h·ª£p
            if spacy_analysis and 'entities' in spacy_analysis:
                corrected_entities = []
                for entity in spacy_analysis['entities']:
                    # S·ª≠a c√°c l·ªói cho vƒÉn b·∫£n h·ªón h·ª£p
                    entity = self.correct_mixed_language_ner_labels(entity, text)
                    corrected_entities.append(entity)
                spacy_analysis['entities'] = corrected_entities
            
            return {
                'language': 'mixed',
                'detected_language': 'mixed',
                'nltk_analysis': {
                    'tokens': nltk_tokens,
                    'pos_tags': nltk_pos_tags
                },
                'spacy_analysis': spacy_analysis
            }
        
        return None
    
    def correct_mixed_language_ner_labels(self, entity, full_text):
        """S·ª≠a c√°c nh√£n NER cho vƒÉn b·∫£n h·ªón h·ª£p"""
        text = entity['text'].lower()
        current_label = entity['label']
        
        # S·ª≠a c√°c l·ªói ph·ªï bi·∫øn cho vƒÉn b·∫£n h·ªón h·ª£p
        if 'joe biden' in text and current_label == 'ORG':
            entity['label'] = 'PERSON'
            entity['description'] = 'Person'
        elif 'ph·∫°m minh ch√≠nh' in text and current_label == 'ORG':
            entity['label'] = 'PER'
            entity['description'] = 'T√™n ng∆∞·ªùi'
        elif 'washington d.c.' in text and current_label == 'PERSON':
            entity['label'] = 'GPE'
            entity['description'] = 'Geopolitical entity'
        elif 'microsoft' in text and current_label == 'PERSON':
            entity['label'] = 'ORG'
            entity['description'] = 'Organization'
        elif 'ph·∫°m nh·∫≠t v∆∞·ª£ng' in text and current_label == 'ORG':
            entity['label'] = 'PER'
            entity['description'] = 'T√™n ng∆∞·ªùi'
        elif 'satya nadella' in text and current_label == 'ORG':
            entity['label'] = 'PERSON'
            entity['description'] = 'Person'
        elif 'h√† n·ªôi' in text and current_label == 'ORG':
            entity['label'] = 'GPE'
            entity['description'] = 'Geopolitical entity'
        elif 'vingroup' in text and current_label == 'PERSON':
            entity['label'] = 'ORG'
            entity['description'] = 'T·ªï ch·ª©c'
        elif 'ai' in text and current_label == 'PERSON':
            entity['label'] = 'MISC'
            entity['description'] = 'Technology'
        elif 'mit' in text and current_label == 'PERSON':
            entity['label'] = 'ORG'
            entity['description'] = 'Organization'
        elif 'nguy·ªÖn kim s∆°n' in text and current_label == 'ORG':
            entity['label'] = 'PER'
            entity['description'] = 'T√™n ng∆∞·ªùi'
        elif 'boston' in text and current_label == 'PERSON':
            entity['label'] = 'GPE'
            entity['description'] = 'Geopolitical entity'
        # Th√™m c√°c s·ª≠a l·ªói cho vƒÉn b·∫£n ti·∫øng Vi·ªát
        elif 'fpt software' in text and current_label == 'PERSON':
            entity['label'] = 'ORG'
            entity['description'] = 'Organization'
        elif 'nguy·ªÖn th√†nh nam' in text and current_label == 'EVENT':
            entity['label'] = 'PERSON'
            entity['description'] = 'Person'
        elif 'ceo' in text and current_label == 'PERSON':
            entity['label'] = 'MISC'
            entity['description'] = 'Job title'
        
        return entity
    
    def analyze_text(self, text: str) -> Optional[Dict]:
        """Ph√¢n t√≠ch vƒÉn b·∫£n ho√†n ch·ªânh v·ªõi h·ªó tr·ª£ ƒëa ng√¥n ng·ªØ"""
        # Validate input
        is_valid, error_msg = self.validate_input(text)
        if not is_valid:
            logger.warning(f"Input validation failed: {error_msg}")
            return None
        
        # Check cache
        text_hash = hash(text.strip())
        if text_hash in self.cache:
            logger.info("Returning cached result")
            return self.cache[text_hash]
        
        try:
            # Ph√°t hi·ªán ng√¥n ng·ªØ
            detected_language = self.cached_detect_language(text)
            
            # Ki·ªÉm tra xem c√≥ ph·∫£i vƒÉn b·∫£n h·ªón h·ª£p kh√¥ng
            mixed_analysis = self.analyze_mixed_language_text(text)
            if mixed_analysis:
                # T√≠nh confidence score
                entities = mixed_analysis.get('spacy_analysis', {}).get('entities', [])
                tokens = mixed_analysis.get('nltk_analysis', {}).get('tokens', [])
                confidence = self.calculate_confidence_score(entities, tokens)
                mixed_analysis['confidence_score'] = confidence
                
                # Cache result
                self.cache[text_hash] = mixed_analysis
                return mixed_analysis
            
            if detected_language == 'vi':
                # Ph√¢n t√≠ch ti·∫øng Vi·ªát
                vietnamese_tokens = self.tokenize_vietnamese(text)
                vietnamese_pos_tags = self.pos_tag_vietnamese(text)
                underthesea_analysis = self.analyze_vietnamese_with_underthesea(text)
                
                result = {
                    'language': 'vietnamese',
                    'detected_language': detected_language,
                    'nltk_analysis': {
                        'tokens': vietnamese_tokens,
                        'pos_tags': vietnamese_pos_tags
                    },
                    'spacy_analysis': underthesea_analysis,
                    'vietnamese_analysis': {
                        'tokens': vietnamese_tokens,
                        'pos_tags': vietnamese_pos_tags,
                        'underthesea_analysis': underthesea_analysis
                    }
                }
                
                # T√≠nh confidence score
                entities = underthesea_analysis.get('entities', []) if underthesea_analysis else []
                confidence = self.calculate_confidence_score(entities, vietnamese_tokens)
                result['confidence_score'] = confidence
                
                # Cache result
                self.cache[text_hash] = result
                return result
            else:
                # Ph√¢n t√≠ch ti·∫øng Anh (ho·∫∑c ng√¥n ng·ªØ kh√°c)
                nltk_tokens = self.tokenize_with_nltk(text)
                nltk_pos_tags = self.pos_tag_with_nltk(nltk_tokens)
                spacy_analysis = self.analyze_with_spacy(text)
                
                result = {
                    'language': 'english',
                    'detected_language': detected_language,
                    'nltk_analysis': {
                        'tokens': nltk_tokens,
                        'pos_tags': nltk_pos_tags
                    },
                    'spacy_analysis': spacy_analysis
                }
                
                # T√≠nh confidence score
                entities = spacy_analysis.get('entities', []) if spacy_analysis else []
                confidence = self.calculate_confidence_score(entities, nltk_tokens)
                result['confidence_score'] = confidence
                
                # Cache result
                self.cache[text_hash] = result
                return result
                
        except Exception as e:
            logger.error(f"Error in analyze_text: {str(e)}")
            return None

# Kh·ªüi t·∫°o analyzer
analyzer = TextAnalyzer()

@app.route('/')
def index():
    """Trang ch·ªß"""
    return render_template('index.html')

@app.route('/analyze', methods=['POST'])
def analyze():
    """API endpoint ƒë·ªÉ ph√¢n t√≠ch vƒÉn b·∫£n"""
    try:
        # Validate request
        if not request.is_json:
            return jsonify({'error': 'Request ph·∫£i l√† JSON'}), 400
        
        data = request.get_json()
        if not data:
            return jsonify({'error': 'D·ªØ li·ªáu JSON kh√¥ng h·ª£p l·ªá'}), 400
        
        text = data.get('text', '').strip()
        
        logger.info(f"Analyzing text: {text[:50]}...")
        
        print(f"\n{'='*60}")
        print(f"üîç PH√ÇN T√çCH VƒÇN B·∫¢N M·ªöI")
        print(f"{'='*60}")
        print(f"üìù VƒÉn b·∫£n ƒë·∫ßu v√†o: {text}")
        print(f"üìè ƒê·ªô d√†i: {len(text)} k√Ω t·ª±")
        
        # Validate input
        is_valid, error_msg = analyzer.validate_input(text)
        if not is_valid:
            print(f"‚ùå L·ªói validation: {error_msg}")
            return jsonify({'error': error_msg}), 400
        
        # Ph√¢n t√≠ch vƒÉn b·∫£n
        result = analyzer.analyze_text(text)
        
        if result is None:
            print("‚ùå L·ªói: Kh√¥ng th·ªÉ ph√¢n t√≠ch vƒÉn b·∫£n")
            return jsonify({'error': 'Kh√¥ng th·ªÉ ph√¢n t√≠ch vƒÉn b·∫£n'}), 500
        
        # Log k·∫øt qu·∫£ chi ti·∫øt
        print(f"\nüåç TH√îNG TIN NG√îN NG·ªÆ:")
        print(f"   - Ng√¥n ng·ªØ: {result.get('language', 'unknown')}")
        print(f"   - Ng√¥n ng·ªØ ph√°t hi·ªán: {result.get('detected_language', 'unknown')}")
        print(f"   - Confidence Score: {result.get('confidence_score', 0.0):.2f}")
        
        # Log tokens
        if 'nltk_analysis' in result:
            tokens = result['nltk_analysis'].get('tokens', [])
            pos_tags = result['nltk_analysis'].get('pos_tags', [])
            print(f"\nüî§ TOKENIZATION:")
            print(f"   - S·ªë tokens: {len(tokens)}")
            print(f"   - Tokens: {tokens}")
            
            print(f"\nüè∑Ô∏è POS TAGS:")
            for token, pos in pos_tags:
                print(f"   - {token}: {pos}")
        
        # Log entities
        if 'spacy_analysis' in result and result['spacy_analysis']:
            entities = result['spacy_analysis'].get('entities', [])
            print(f"\nüéØ NAMED ENTITY RECOGNITION:")
            print(f"   - S·ªë entities: {len(entities)}")
            for entity in entities:
                print(f"   - {entity['text']} ({entity['label']}): {entity['description']}")
        else:
            print(f"\nüéØ NAMED ENTITY RECOGNITION:")
            print(f"   - Kh√¥ng t√¨m th·∫•y entities")
        
        # Log ph√¢n t√≠ch chi ti·∫øt
        if 'spacy_analysis' in result and result['spacy_analysis']:
            tokens_with_pos = result['spacy_analysis'].get('tokens_with_pos', [])
            print(f"\nüìä PH√ÇN T√çCH CHI TI·∫æT:")
            print(f"   - S·ªë tokens chi ti·∫øt: {len(tokens_with_pos)}")
            for token_info in tokens_with_pos:
                print(f"   - {token_info['token']}: POS={token_info['pos']}, Tag={token_info['tag']}, Lemma={token_info['lemma']}")
        
        print(f"\n‚úÖ PH√ÇN T√çCH HO√ÄN T·∫§T")
        print(f"{'='*60}\n")
        
        return jsonify({
            'success': True,
            'result': result
        })
    
    except Exception as e:
        print(f"\n‚ùå L·ªñI KHI PH√ÇN T√çCH: {str(e)}")
        print(f"{'='*60}\n")
        return jsonify({'error': f'L·ªói khi ph√¢n t√≠ch: {str(e)}'}), 500

@app.route('/health')
def health():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'spacy_available': nlp is not None,
        'cache_size': len(analyzer.cache)
    })

@app.route('/cache/clear', methods=['POST'])
def clear_cache():
    """Clear analysis cache"""
    try:
        cache_size = len(analyzer.cache)
        analyzer.cache.clear()
        logger.info(f"Cache cleared. Removed {cache_size} entries.")
        return jsonify({
            'success': True,
            'message': f'ƒê√£ x√≥a {cache_size} entries kh·ªèi cache',
            'cleared_entries': cache_size
        })
    except Exception as e:
        logger.error(f"Error clearing cache: {str(e)}")
        return jsonify({'error': f'L·ªói khi x√≥a cache: {str(e)}'}), 500

@app.route('/cache/stats')
def cache_stats():
    """Get cache statistics"""
    return jsonify({
        'cache_size': len(analyzer.cache),
        'confidence_threshold': analyzer.confidence_threshold
    })

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)
